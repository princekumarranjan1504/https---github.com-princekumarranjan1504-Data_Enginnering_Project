{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880078db",
   "metadata": {},
   "source": [
    "# NeoStats — Documentation\n",
    "\n",
    "This notebook contains the architecture, data flow, data model, assumptions, data cleaning and transformation logic, and suggested Power BI visuals for the Neostars_Project deliverable. Use this as the formal documentation artifact for the data engineering pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46b7a3",
   "metadata": {},
   "source": [
    "## 1 — Architecture (logical)\n",
    "\n",
    "A lightweight batch pipeline designed to run locally or be mapped to Azure services. The components are:\n",
    "\n",
    "- Data sources: Excel workbook with sheets: `Server_Metadata`, `Server_Performance_Station1`, `Server_Performance_Station2`\n",
    "- Ingestion: `scripts/ingestion.py` (reads the Excel, returns DataFrames)\n",
    "- Cleaning: `scripts/cleaning.py` (normalizes headers, cleans, detects anomalies)\n",
    "- Transformation: `scripts/transformation.py` (computes metrics, aggregates, enriches metadata)\n",
    "- Storage/Output: `../output/structured_data.csv` and `../output/aggregates_<window>.csv`\n",
    "- Visualization: Power BI using the CSVs or a database sink.\n",
    "\n",
    "ASCII diagram (logical):\n",
    "\n",
    "```\n",
    "[Excel (.xlsx)]\n",
    "      |\n",
    "      v\n",
    "[scripts/ingestion.py] -> raw DataFrames\n",
    "      |\n",
    "      v\n",
    "[scripts/cleaning.py] -> cleaned DataFrame + anomalies log\n",
    "      |\n",
    "      v\n",
    "[scripts/transformation.py] -> enriched DataFrame + aggregates (1min/5min/1H)\n",
    "      |\n",
    "      v\n",
    "[../output/*.csv] -> Power BI (PBIX)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309c626",
   "metadata": {},
   "source": [
    "## 2 — Data flow (step-by-step)\n",
    "\n",
    "1. `ingestion.py` reads the Excel workbook using `pandas.read_excel` and returns three DataFrames: metadata, st1, st2.\n",
    "2. Notebook concatenates station tables into a single performance table (`pd.concat([st1, st2])`).\n",
    "3. `clean_performance_data` is called with the combined table. It performs header normalization, drops irrelevant columns, fills or imputes numeric nulls, parses timestamps and detects anomalies (missing timestamps, negative values, duplicates). It returns the cleaned DataFrame.\n",
    "4. `transform_data` computes CPU and Memory utilization, attempts to compute Disk I/O and Network rates (either from provided rate columns or by differencing cumulative counters per server), enriches rows by joining with `Server_Metadata`, and writes aggregated CSVs for 1min/5min/hourly windows.\n",
    "5. Final enriched dataset is saved to `../output/structured_data.csv` and can be loaded into Power BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fd678",
   "metadata": {},
   "source": [
    "## 3 — Data model (recommended tables and schemas)\n",
    "\n",
    "Recommended table schemas (denormalized for BI):\n",
    "\n",
    "### `servers_metadata` (from `Server_Metadata` sheet)\n",
    "- server_id (PK)\n",
    "- cluster\n",
    "- admin_contact\n",
    "- location\n",
    "- os_type\n",
    "- instance_size\n",
    "- other_attributes...\n",
    "\n",
    "### `server_performance` (cleaned, row-level time series)\n",
    "- server_id\n",
    "- timestamp (UTC)\n",
    "- cpu_utilization (percent)\n",
    "- memory_utilization (percent)\n",
    "- disk_io_rate_mb_s\n",
    "- network_in_kbps\n",
    "- network_out_kbps\n",
    "- anomaly_flag\n",
    "- imputed_<col> flags (boolean) for traceability\n",
    "- source_station\n",
    "\n",
    "### `aggregates_<window>` (produced CSVs)\n",
    "- server_id\n",
    "- window_start\n",
    "- avg_cpu\n",
    "- p95_cpu (optional enhancement)\n",
    "- avg_mem\n",
    "- avg_disk_io_mb_s\n",
    "- avg_net_in_kBps\n",
    "- avg_net_out_kBps\n",
    "\n",
    "These tables are written as CSV in `../output/` by the pipeline and can be swapped for Parquet/Delta/SQL when scaling to cloud storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdb264",
   "metadata": {},
   "source": [
    "## 4 — Assumptions\n",
    "\n",
    "- The Excel file uses the sheet names described in the prompt: `Server_Metadata`, `Server_Performance_Station1`, `Server_Performance_Station2`.\n",
    "- Timestamps are present or parsable; when not, rows are flagged and logged by the cleaning step.\n",
    "- CPU/Memory metrics may be provided either as 'used' and 'total' columns or pre-computed percentages; the code detects both forms.\n",
    "- Disk and network may be provided as instantaneous rates (MB/s, kB/s) or cumulative counters (bytes). When counters are present, the pipeline computes rates by per-server differencing and time-deltas.\n",
    "- The pipeline is batch-oriented (not streaming). For production, streaming ingestion would use Kafka/EventHub + stream processing.\n",
    "- Missing/invalid numeric values can be imputed with median or zero depending on policy; the cleaning function records imputation flags for traceability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ad4cd",
   "metadata": {},
   "source": [
    "## 5 — Cleaning logic (summary)\n",
    "\n",
    "Implemented in `scripts/cleaning.py`. Key points:\n",
    "\n",
    "- Header normalization: the cleaner normalizes header names by removing non-alphanumerics and lowercasing, then auto-maps common variants to canonical names (e.g., `CPU Usage (%)` -> `CPU_Utilization`).\n",
    "- Drop irrelevant columns: `Config_Version`, `Last_Patch_Date`, `Deployment_Token` are removed if present.\n",
    "- Numeric null handling: the cleaner fills selected numeric columns with 0 where appropriate and logs the action. It also supports imputation behaviors (median/zero) in enhanced versions.\n",
    "- Timestamp parsing: attempts to parse `Timestamp` or variants (e.g., `Log_Timestamp`) and coerces invalid values to NaT; these are counted/printed as anomalies.\n",
    "- Negative values: detected and corrected (converted to absolute or set to NaN depending on policy).\n",
    "- Duplicates: detected and dropped; duplicates are counted and printed in the anomaly summary.\n",
    "- Output: a cleaned DataFrame is returned and the cleaner prints an anomaly summary suitable for audit or saved logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b90b96",
   "metadata": {},
   "source": [
    "## 6 — Transformation logic (summary)\n",
    "\n",
    "Implemented in `scripts/transformation.py`. Key points:\n",
    "\n",
    "- CPU/Memory Utilization: computed when 'used' and 'total' columns are present, or used directly when a percentage column is available. The result is `CPU_Utilization` and `Memory_Utilization` in percent.\n",
    "- Disk I/O Rate: uses an existing rate column when present, otherwise attempts to compute per-server rate by differencing cumulative counters (read/write bytes) and dividing by time delta, converted to MB/s.\n",
    "- Network Throughput: similarly computed from cumulative counters (diff + time delta) or used directly when rate columns exist; results stored as `Network_In_kBps` and `Network_Out_kBps`.\n",
    "- Enrichment: joined with `Server_Metadata` on `Server_ID` to add `location`, `os_type`, `instance_size`, and `cluster` for each performance row.\n",
    "- Aggregations: windowed (1min, 5min, 1H) mean aggregations are produced per `Server_ID` and saved as CSVs in `../output/aggregates_<window>.csv` for use by Power BI.\n",
    "- Output: the enriched performance DataFrame saved to `../output/structured_data.csv` (via notebook or utils.save_output).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff9f41",
   "metadata": {},
   "source": [
    "## 7 — Visual insights & Power BI guidance\n",
    "\n",
    "Suggested KPIs and visuals:\n",
    "\n",
    "- KPI cards: Average CPU Utilization (cluster-level), % uptime (derived), Number of High-Load servers.\n",
    "- Time series: CPU and memory trend per server or cluster (use aggregates_1min/5min for smoother charts).\n",
    "- Heatmap / Matrix: CPU utilization by cluster vs. server to quickly identify hotspots.\n",
    "- Top-N lists: Top 10 servers by average CPU or by peak memory usage over selected window.\n",
    "- Drill-through: From cluster view to server detail with server metadata (OS, instance size, admin contact).\n",
    "- Slicers/filters: Time window (last 24h, last 7d), Cluster, Server, OS type.\n",
    "\n",
    "Power BI data sources: point Power BI at `../output/aggregates_hourly.csv` (or load `structured_data.csv` for row-level analysis). For better performance, switch to a parquet/SQL source in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99a7f6",
   "metadata": {},
   "source": [
    "## 8 — How to run the pipeline locally\n",
    "\n",
    "Open `notebooks/neo_pipeline.ipynb` and run the cells in order, OR run the short script below in PowerShell at the project root. This will read the Excel in `./data/`, clean, transform, and write outputs to `./output/`.\n",
    "\n",
    "```powershell\n",
    "# From the project root (PowerShell)\n",
    "python - <<'PY'\n",
    "from scripts.ingestion import load_excel\n",
    "from scripts.cleaning import clean_performance_data\n",
    "from scripts.transformation import transform_data\n",
    "from scripts.utils import save_output\n",
    "import pandas as pd\n",
    "meta, st1, st2 = load_excel('data/Data Engineering Use Case Dataset.xlsx')\n",
    "perf = pd.concat([st1, st2], ignore_index=True)\n",
    "cleaned = clean_performance_data(perf)\n",
    "final = transform_data(cleaned, meta)\n",
    "save_output(final, 'output/structured_data.csv')\n",
    "print(final.head())\n",
    "PY\n",
    "```\n",
    "\n",
    "If you prefer to run the notebook, the `neo_pipeline.ipynb` already includes the same steps and a logger setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f01747",
   "metadata": {},
   "source": [
    "## 9 — Artifacts produced\n",
    "\n",
    "- `../output/structured_data.csv` — cleaned, enriched row-level dataset ready for BI.\n",
    "- `../output/aggregates_1min.csv`, `../output/aggregates_5min.csv`, `../output/aggregates_hourly.csv` — aggregated time-series views.\n",
    "- Power BI report (deliverable) — build on the aggregates or the row-level CSV.\n",
    "- Notebook: this document (`notebooks/documentation.ipynb`) — architecture, data flow, model, and run instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28226cda",
   "metadata": {},
   "source": [
    "## 10 — Next steps and recommendations\n",
    "\n",
    "1. Validate results against the sample dataset and inspect anomalies reported by `scripts/cleaning.py`.\n",
    "2. Convert CSV aggregates to Parquet or load them into a SQL/Delta store for scalability.\n",
    "3. If Azure is available: replace local steps with Azure Data Factory (ingestion), ADLS Gen2 (storage), Azure Synapse or Databricks (transformation), and Power BI Service for report publishing.\n",
    "4. Add unit tests for cleaning and transformation functions, including edge cases (missing timestamps, counter rollover, duplicate detection).\n",
    "5. Create a small CI pipeline to run the notebooks or tests on commit and validate data regression.\n",
    "\n",
    "---\n",
    "Documentation complete."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
