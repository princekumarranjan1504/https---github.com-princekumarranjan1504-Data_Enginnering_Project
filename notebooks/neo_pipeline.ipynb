{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b352d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\prince\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\prince\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# notebooks/neo_pipeline.ipynb\n",
    "\n",
    "# Ensure the project root is available to the notebook.\n",
    "# The notebook adds the project root to sys.path below so local imports like `scripts.*` work.\n",
    "# Install only the runtime dependency needed to read .xlsx files. If you want an editable\n",
    "# install (pip -e), add a minimal pyproject.toml or setup.py to the project root and run\n",
    "# that command manually. The editable install often fails when the project is not packaged.\n",
    "\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb302edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # notebooks/ -> project root\n",
    "project_root_str = str(project_root)\n",
    "if project_root_str not in sys.path:\n",
    "    sys.path.insert(0, project_root_str)\n",
    "\n",
    "from scripts.ingestion import load_excel\n",
    "from scripts.cleaning import clean_performance_data\n",
    "from scripts.transformation import transform_data\n",
    "# Import and reload utils to ensure latest definitions are available in the notebook kernel\n",
    "import importlib\n",
    "import scripts.utils as utils\n",
    "importlib.reload(utils)\n",
    "from scripts.utils import save_output, log_message, setup_logger\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a5bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 17:15:12,522 | INFO | Logger initialized.\n",
      "2025-10-30 17:15:12,525 | INFO | Starting NeoStats Data Pipeline...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting NeoStats Data Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 17:15:17,702 | INFO | Data ingestion completed.\n",
      "2025-10-30 17:15:17,726 | INFO | Station data combined\n",
      "2025-10-30 17:15:17,804 | INFO | Data cleaning completed.\n",
      "2025-10-30 17:15:17,836 | INFO | Data transformation completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded successfully:\n",
      " - Metadata shape: (100, 9)\n",
      " - Station1 shape: (3000, 14)\n",
      " - Station2 shape: (2000, 11)\n",
      "INFO: Data ingestion completed.\n",
      "Combined station data shape: (5000, 14)\n",
      "INFO: Station data combined\n",
      "Dropped irrelevant columns (if any).\n",
      "Filled 520 missing values in Memory_Usage (%) with 0.\n",
      "Filled 271 missing values in Disk_IO (%) with 0.\n",
      "Filled 208 missing values in Network_Traffic_Out (MB/s) with 0.\n",
      "Unified schema and sorted columns.\n",
      "Final cleaned data: 5000 rows, 11 columns\n",
      "No major anomalies detected.\n",
      "INFO: Data cleaning completed.\n",
      "Starting transformation...\n",
      "CPU Utilization column ready.\n",
      "Memory Utilization column ready.\n",
      "Disk I/O Rate column ready.\n",
      "Calculated Network Throughput (average of In/Out).\n",
      "Merged performance data with metadata on Server_ID.\n",
      "Added Anomaly_Flag based on CPU Utilization > 85%.\n",
      "Transformation complete: rows = 5000 columns = 21\n",
      "INFO: Data transformation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 17:15:18,047 | INFO | Output saved successfully: ../output/structured_data.csv\n",
      "2025-10-30 17:15:18,049 | INFO | Pipeline executed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to ../output/structured_data.csv\n",
      "INFO: Pipeline executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "setup_logger(\"../logs/pipeline.log\")\n",
    "\n",
    "log_message(\"Starting NeoStats Data Pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Load Excel data\n",
    "    file_path = \"../data/Data Engineering Use Case Dataset.xlsx\"\n",
    "    metadata, st1, st2 = load_excel(file_path)\n",
    "\n",
    "    # Check if sheets loaded correctly\n",
    "    if metadata is None or st1 is None or st2 is None:\n",
    "        raise ValueError(\"Failed to load one or more sheets from the Excel file.\")\n",
    "\n",
    "    log_message(\"Data ingestion completed.\")\n",
    "\n",
    "    # Step 2: Combine station performance tables into one DataFrame\n",
    "    perf_data = pd.concat([st1, st2], ignore_index=True)\n",
    "    print('Combined station data shape:', perf_data.shape)\n",
    "    log_message('Station data combined')\n",
    "\n",
    "    # Step 3: Clean performance data (call the cleaning function)\n",
    "    cleaned_data = clean_performance_data(perf_data)\n",
    "    log_message(\"Data cleaning completed.\")\n",
    "\n",
    "    # Step 4: Transform and merge with metadata\n",
    "    final_data = transform_data(cleaned_data, metadata)\n",
    "    log_message(\"Data transformation completed.\")\n",
    "\n",
    "    # Step 5: Save final output to CSV\n",
    "    save_output(final_data, \"../output/structured_data.csv\")\n",
    "    log_message(\"Pipeline executed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # For printing and log a short message\n",
    "    log_message(\"Pipeline failed: \" + str(e), level=\"error\")\n",
    "    print('Pipeline failed. See logs for details.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
